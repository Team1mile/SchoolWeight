sum_y <- 0
new_year <- NULL
new_age <- NULL
new_sex <- NULL
new_weight <- NULL
new_theoCount <- NULL
new_diffCount <- NULL
while (i <= 70){
tmp_count <- pJohnson(i+0.5, parms) - pJohnson(i-0.5, parms)
tmp_count <- round(tmp_count * sum(hoken_data$Count) * 10)
if (tmp_count > 0){
y <- c(y, rep(i, tmp_count))
sum_y <- sum_y + tmp_count
}
new_year <- c(new_year, c(current_year))
new_age <- c(new_age, c(current_age))
new_sex <- c(new_sex, c(current_sex))
new_weight <- c(new_weight, c(i))
new_theoCount <- c(new_theoCount, c(tmp_count))
hoken_data_year <- hoken_data[hoken_data$Year==current_year,]
hoken_data_year_age <- hoken_data_year[hoken_data_year$Age==current_age,]
hoken_data_year_age_sex <- hoken_data_year_age[hoken_data_year_age$Sex==current_sex,]
hoken_data_year_age_sex_weight <- hoken_data_year_age_sex[hoken_data_year_age_sex$Weight==i,]
if (nrow(hoken_data_year_age_sex_weight) > 0){
new_diff = c(new_diff, c(abs(hoken_data_year_age_sex_weight[1,]$Count - new_theoCount)))
}
else {
new_diff = c(new_diff, c(hoken_data_year_age_sex_weight[1,]$Count))
}
i <- i + 1
}
sum_x <- sum(hoken_data$Count) * 10
hoken_theo_data <- data.frame(Year = new_year, Age = new_age, Sex = new_sex, Weight = new_weight, Theo_count = new_theocount, Diff_count = new_diffcount)
library(moments)
library(SuppDists)
current_year <- 2016
current_sex <- "Male"
current_age <- 9
# Reading weight data
hoken_data <- read.csv(file = "C:\\Users\\mackun\\Documents\\SchoolWeight\\Hoken_data.csv", header = TRUE, sep = ",")
# Reading mean & SD data
hoken_meansd <- read.csv(file="C:\\Users\\mackun\\Documents\\SchoolWeight\\Hoken_meansd.csv", header = TRUE, sep = ",")
i <- 1
x <- NULL
while (i <= nrow(hoken_data)){
x <- c(x, rep(hoken_data[i,]$Weight, hoken_data[i,]$Count * 10))
i <- i + 1
}
cur_kur <- kurtosis(x)
cur_skw <- skewness(x)
hoken_meansd_year <- hoken_meansd[hoken_meansd$Year==current_year,]
hoken_meansd_year_age <- hoken_meansd_year[hoken_meansd_year$Age==current_age,]
hoken_meansd_year_age_sex <- hoken_meansd_year_age[hoken_meansd_year_age$Sex==current_sex,]
cur_sd <- hoken_meansd_year_age_sex$SD
cur_var <- cur_sd * cur_sd
cur_mean <- hoken_meansd_year_age_sex$Mean
parms <- JohnsonFit(x)
i <- 10
y <- NULL
sum_y <- 0
new_year <- NULL
new_age <- NULL
new_sex <- NULL
new_weight <- NULL
new_theoCount <- NULL
new_diffCount <- NULL
while (i <= 70){
tmp_count <- pJohnson(i+0.5, parms) - pJohnson(i-0.5, parms)
tmp_count <- round(tmp_count * sum(hoken_data$Count) * 10)
if (tmp_count > 0){
y <- c(y, rep(i, tmp_count))
sum_y <- sum_y + tmp_count
}
new_year <- c(new_year, c(current_year))
new_age <- c(new_age, c(current_age))
new_sex <- c(new_sex, c(current_sex))
new_weight <- c(new_weight, c(i))
new_theoCount <- c(new_theoCount, c(tmp_count))
hoken_data_year <- hoken_data[hoken_data$Year==current_year,]
hoken_data_year_age <- hoken_data_year[hoken_data_year$Age==current_age,]
hoken_data_year_age_sex <- hoken_data_year_age[hoken_data_year_age$Sex==current_sex,]
hoken_data_year_age_sex_weight <- hoken_data_year_age_sex[hoken_data_year_age_sex$Weight==i,]
if (nrow(hoken_data_year_age_sex_weight) > 0){
new_diffCount = c(new_diff, c(abs(hoken_data_year_age_sex_weight[1,]$Count - new_theoCount)))
}
else {
new_diffCount = c(new_diff, c(hoken_data_year_age_sex_weight[1,]$Count))
}
i <- i + 1
}
sum_x <- sum(hoken_data$Count) * 10
hoken_theo_data <- data.frame(Year = new_year, Age = new_age, Sex = new_sex, Weight = new_weight, Theo_count = new_theocount, Diff_count = new_diffCount)
library(moments)
library(SuppDists)
current_year <- 2016
current_sex <- "Male"
current_age <- 9
# Reading weight data
hoken_data <- read.csv(file = "C:\\Users\\mackun\\Documents\\SchoolWeight\\Hoken_data.csv", header = TRUE, sep = ",")
# Reading mean & SD data
hoken_meansd <- read.csv(file="C:\\Users\\mackun\\Documents\\SchoolWeight\\Hoken_meansd.csv", header = TRUE, sep = ",")
i <- 1
x <- NULL
while (i <= nrow(hoken_data)){
x <- c(x, rep(hoken_data[i,]$Weight, hoken_data[i,]$Count * 10))
i <- i + 1
}
cur_kur <- kurtosis(x)
cur_skw <- skewness(x)
hoken_meansd_year <- hoken_meansd[hoken_meansd$Year==current_year,]
hoken_meansd_year_age <- hoken_meansd_year[hoken_meansd_year$Age==current_age,]
hoken_meansd_year_age_sex <- hoken_meansd_year_age[hoken_meansd_year_age$Sex==current_sex,]
cur_sd <- hoken_meansd_year_age_sex$SD
cur_var <- cur_sd * cur_sd
cur_mean <- hoken_meansd_year_age_sex$Mean
parms <- JohnsonFit(x)
i <- 10
y <- NULL
sum_y <- 0
new_year <- NULL
new_age <- NULL
new_sex <- NULL
new_weight <- NULL
new_theoCount <- NULL
new_diffCount <- NULL
while (i <= 70){
tmp_count <- pJohnson(i+0.5, parms) - pJohnson(i-0.5, parms)
tmp_count <- round(tmp_count * sum(hoken_data$Count) * 10)
if (tmp_count > 0){
y <- c(y, rep(i, tmp_count))
sum_y <- sum_y + tmp_count
}
new_year <- c(new_year, c(current_year))
new_age <- c(new_age, c(current_age))
new_sex <- c(new_sex, c(current_sex))
new_weight <- c(new_weight, c(i))
new_theoCount <- c(new_theoCount, c(tmp_count))
hoken_data_year <- hoken_data[hoken_data$Year==current_year,]
hoken_data_year_age <- hoken_data_year[hoken_data_year$Age==current_age,]
hoken_data_year_age_sex <- hoken_data_year_age[hoken_data_year_age$Sex==current_sex,]
hoken_data_year_age_sex_weight <- hoken_data_year_age_sex[hoken_data_year_age_sex$Weight==i,]
if (nrow(hoken_data_year_age_sex_weight) > 0){
new_diffCount = c(new_diffCount, c(abs(hoken_data_year_age_sex_weight[1,]$Count - new_theoCount)))
}
else {
new_diffCount = c(new_diffCount, c(hoken_data_year_age_sex_weight[1,]$Count))
}
i <- i + 1
}
sum_x <- sum(hoken_data$Count) * 10
hoken_theo_data <- data.frame(Year = new_year, Age = new_age, Sex = new_sex, Weight = new_weight, Theo_count = new_theocount, Diff_count = new_diffCount)
hoken_theo_data <- data.frame(Year = new_year, Age = new_age, Sex = new_sex, Weight = new_weight, Theo_count = new_theoCount, Diff_count = new_diffCount)
library(moments)
library(SuppDists)
current_year <- 2016
current_sex <- "Male"
current_age <- 9
# Reading weight data
hoken_data <- read.csv(file = "C:\\Users\\mackun\\Documents\\SchoolWeight\\Hoken_data.csv", header = TRUE, sep = ",")
# Reading mean & SD data
hoken_meansd <- read.csv(file="C:\\Users\\mackun\\Documents\\SchoolWeight\\Hoken_meansd.csv", header = TRUE, sep = ",")
i <- 1
x <- NULL
while (i <= nrow(hoken_data)){
x <- c(x, rep(hoken_data[i,]$Weight, hoken_data[i,]$Count * 10))
i <- i + 1
}
cur_kur <- kurtosis(x)
cur_skw <- skewness(x)
hoken_meansd_year <- hoken_meansd[hoken_meansd$Year==current_year,]
hoken_meansd_year_age <- hoken_meansd_year[hoken_meansd_year$Age==current_age,]
hoken_meansd_year_age_sex <- hoken_meansd_year_age[hoken_meansd_year_age$Sex==current_sex,]
cur_sd <- hoken_meansd_year_age_sex$SD
cur_var <- cur_sd * cur_sd
cur_mean <- hoken_meansd_year_age_sex$Mean
parms <- JohnsonFit(x)
i <- 10
y <- NULL
sum_y <- 0
new_year <- NULL
new_age <- NULL
new_sex <- NULL
new_weight <- NULL
new_theoCount <- NULL
new_diffCount <- NULL
while (i <= 70){
tmp_count <- pJohnson(i+0.5, parms) - pJohnson(i-0.5, parms)
tmp_count <- round(tmp_count * sum(hoken_data$Count) * 10)
if (tmp_count > 0){
y <- c(y, rep(i, tmp_count))
sum_y <- sum_y + tmp_count
}
new_year <- c(new_year, c(current_year))
new_age <- c(new_age, c(current_age))
new_sex <- c(new_sex, c(current_sex))
new_weight <- c(new_weight, c(i))
new_theoCount <- c(new_theoCount, c(tmp_count))
hoken_data_year <- hoken_data[hoken_data$Year==current_year,]
hoken_data_year_age <- hoken_data_year[hoken_data_year$Age==current_age,]
hoken_data_year_age_sex <- hoken_data_year_age[hoken_data_year_age$Sex==current_sex,]
hoken_data_year_age_sex_weight <- hoken_data_year_age_sex[hoken_data_year_age_sex$Weight==i,]
if (nrow(hoken_data_year_age_sex_weight) > 0){
new_diffCount = c(new_diffCount, c(abs(hoken_data_year_age_sex_weight[1,]$Count - new_theoCount)))
}
else {
new_diffCount = c(new_diffCount, c(hoken_data_year_age_sex_weight[1,]$Count))
}
i <- i + 1
}
sum_x <- sum(hoken_data$Count) * 10
hoken_theo_data <- data.frame(Year = new_year, Age = new_age, Sex = new_sex, Weight = new_weight, Theo_count = new_theoCount, Diff_count = new_diffCount)
View(hoken_theo_data)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
# Library install
library(moments)
library(SuppDists)
# どの年度、性別、年齢でデータ統計処理を試すかの定義
current_year <- 2016
current_sex <- "Male"
current_age <- 9
# Reading weight data
hoken_data <- read.csv(file = "C:\\Users\\mackun\\Documents\\SchoolWeight\\Hoken_data.csv", header = TRUE, sep = ",")
# Reading mean & SD data
hoken_meansd <- read.csv(file="C:\\Users\\mackun\\Documents\\SchoolWeight\\Hoken_meansd.csv", header = TRUE, sep = ",")
# 体重別の度数データから一件別のList型データに展開(R関数が使いやすいように)
i <- 1
x <- NULL
while (i <= nrow(hoken_data)){
x <- c(x, rep(hoken_data[i,]$Weight, hoken_data[i,]$Count * 10))
i <- i + 1
}
# 尖度・歪度を求める
cur_kur <- kurtosis(x)
cur_skw <- skewness(x)
# CSVから読み込んだ平均・標準偏差データから該当の年度・性別・年齢のデータを抽出
hoken_meansd_year <- hoken_meansd[hoken_meansd$Year==current_year,]
hoken_meansd_year_age <- hoken_meansd_year[hoken_meansd_year$Age==current_age,]
hoken_meansd_year_age_sex <- hoken_meansd_year_age[hoken_meansd_year_age$Sex==current_sex,]
cur_sd <- hoken_meansd_year_age_sex$SD
cur_var <- cur_sd * cur_sd
cur_mean <- hoken_meansd_year_age_sex$Mean
# ジョンソンSU分布への適用結果を補完するための準備
i <- 10
y <- NULL
sum_y <- 0
new_year <- NULL
new_age <- NULL
new_sex <- NULL
new_weight <- NULL
new_theoCount <- NULL
new_diffCount <- NULL
# ジョンソンSU分布への適用を試みる
parms <- JohnsonFit(x)
while (i <= 70){
# ジョンソンSU分布を適用した時の度数理論値を計算
# (累積分布関数のi+0.5の値からi-0.5の値を引いて出た値を採用)
tmp_count <- pJohnson(i+0.5, parms) - pJohnson(i-0.5, parms)
tmp_count <- round(tmp_count * sum(hoken_data$Count) * 10)
# ジョンソンSU分布適用時のヒストグラムを作成するためのList型データ作成
if (tmp_count > 0){
y <- c(y, rep(i, tmp_count))
sum_y <- sum_y + tmp_count
}
# 度数の理論値を補完するためのデータフレームを作るためのList型データ準備
new_year <- c(new_year, c(current_year))
new_age <- c(new_age, c(current_age))
new_sex <- c(new_sex, c(current_sex))
new_weight <- c(new_weight, c(i))
new_theoCount <- c(new_theoCount, c(tmp_count))
hoken_data_year <- hoken_data[hoken_data$Year==current_year,]
hoken_data_year_age <- hoken_data_year[hoken_data_year$Age==current_age,]
hoken_data_year_age_sex <- hoken_data_year_age[hoken_data_year_age$Sex==current_sex,]
hoken_data_year_age_sex_weight <- hoken_data_year_age_sex[hoken_data_year_age_sex$Weight==i,]
if (nrow(hoken_data_year_age_sex_weight) > 0){
new_diffCount = c(new_diffCount, c(abs(hoken_data_year_age_sex_weight[1,]$Count - new_theoCount)))
}
else {
new_diffCount = c(new_diffCount, c(hoken_data_year_age_sex_weight[1,]$Count))
}
i <- i + 1
}
sum_x <- sum(hoken_data$Count) * 10
hoken_theo_data <- data.frame(Year = new_year, Age = new_age, Sex = new_sex, Weight = new_weight, Theo_count = new_theoCount, Diff_count = new_diffCount)
# Library install
library(moments)
library(SuppDists)
# どの年度、性別、年齢でデータ統計処理を試すかの定義
current_year <- 2016
current_sex <- "Male"
current_age <- 9
# Reading weight data
hoken_data <- read.csv(file = "C:\\Users\\mackun\\Documents\\SchoolWeight\\Hoken_data.csv", header = TRUE, sep = ",")
# Reading mean & SD data
hoken_meansd <- read.csv(file="C:\\Users\\mackun\\Documents\\SchoolWeight\\Hoken_meansd.csv", header = TRUE, sep = ",")
# 体重別の度数データから一件別のList型データに展開(R関数が使いやすいように)
i <- 1
x <- NULL
while (i <= nrow(hoken_data)){
x <- c(x, rep(hoken_data[i,]$Weight, hoken_data[i,]$Count * 10))
i <- i + 1
}
# 尖度・歪度を求める
cur_kur <- kurtosis(x)
cur_skw <- skewness(x)
# CSVから読み込んだ平均・標準偏差データから該当の年度・性別・年齢のデータを抽出
hoken_meansd_year <- hoken_meansd[hoken_meansd$Year==current_year,]
hoken_meansd_year_age <- hoken_meansd_year[hoken_meansd_year$Age==current_age,]
hoken_meansd_year_age_sex <- hoken_meansd_year_age[hoken_meansd_year_age$Sex==current_sex,]
cur_sd <- hoken_meansd_year_age_sex$SD
cur_var <- cur_sd * cur_sd
cur_mean <- hoken_meansd_year_age_sex$Mean
# ジョンソンSU分布への適用結果を補完するための準備
i <- 10
y <- NULL
sum_y <- 0
new_year <- NULL
new_age <- NULL
new_sex <- NULL
new_weight <- NULL
new_theoCount <- NULL
new_diffCount <- NULL
# ジョンソンSU分布への適用を試みる
parms <- JohnsonFit(x)
while (i <= 70){
# ジョンソンSU分布を適用した時の度数理論値を計算
# (累積分布関数のi+0.5の値からi-0.5の値を引いて出た値を採用)
tmp_count <- pJohnson(i+0.5, parms) - pJohnson(i-0.5, parms)
tmp_count <- round(tmp_count * sum(hoken_data$Count) * 10)
# ジョンソンSU分布適用時のヒストグラムを作成するためのList型データ作成
if (tmp_count > 0){
y <- c(y, rep(i, tmp_count))
sum_y <- sum_y + tmp_count
}
# 度数の理論値を補完するためのデータフレームを作るためのList型データ準備
new_year <- c(new_year, c(current_year))
new_age <- c(new_age, c(current_age))
new_sex <- c(new_sex, c(current_sex))
new_weight <- c(new_weight, c(i))
new_theoCount <- c(new_theoCount, c(tmp_count))
hoken_data_year <- hoken_data[hoken_data$Year==current_year,]
hoken_data_year_age <- hoken_data_year[hoken_data_year$Age==current_age,]
hoken_data_year_age_sex <- hoken_data_year_age[hoken_data_year_age$Sex==current_sex,]
hoken_data_year_age_sex_weight <- hoken_data_year_age_sex[hoken_data_year_age_sex$Weight==i,]
if (nrow(hoken_data_year_age_sex_weight) > 0){
new_diffCount = c(new_diffCount, c(abs(hoken_data_year_age_sex_weight[1,]$Count - new_theoCount)))
}
else {
new_diffCount = c(new_diffCount, c(hoken_data_year_age_sex_weight[1,]$Count))
}
i <- i + 1
}
sum_x <- sum(hoken_data$Count) * 10
hoken_theo_data <- data.frame(Year = new_year, Age = new_age, Sex = new_sex, Weight = new_weight, Theo_count = new_theoCount, Diff_count = new_diffCount)
head(hoken_theo_data)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
length(new_weight)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
View(hoken_theo_data)
View(hoken_theo_data)
hist(x)
hist(x, breaks=50)
curve(y, add=TRUE)
i <- 17:70
plot(i, djohnson(i, parms), add=TRUE)
plot(i, dJohnson(i, parms), add=TRUE)
plot(i, dJohnson(i, parms))
hist(x, breaks=50, add=TRUE)
hist(x, breaks=50, add=T)
hist(x, breaks=50)
hist(y, breaks=50, add=T)
hist(x, breaks=seq(11,70,1), col="#FF00007F", xlim=c(0,100), main="", xlab="" )
hist(y, breaks=seq(11,70,1), col="#0000FF7F", add=T)
hist(x, breaks=seq(11,70,1), col="#FF00007F", xlim=c(10,70), main="", xlab="" )
hist(y, breaks=seq(11,70,1), col="#0000FF7F", add=T)
hist(x, breaks=seq(15,65,1), col="#FF00007F", xlim=c(15,65), main="", xlab="" )
hist(y, breaks=seq(15,65,1), col="#0000FF7F", add=T)
hist(x, breaks=seq(15,65,1), col="#FF00007F")
hist(x, breaks=seq(10,70,1), col="#FF00007F")
hist(y, breaks=seq(10,70,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,70,1), col="#FF00007F")
hist(y, breaks=seq(10,70,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
View(hoken_theo_data)
View(hoken_theo_data)
hist(x, breaks=seq(10,70,1), col="#FF00007F")
hist(y, breaks=seq(10,70,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
View(hoken_theo_data)
View(hoken_theo_data)
View(hoken_data_year_age_sex)
View(hoken_data_year_age_sex)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,70,1), col="#FF00007F")
hist(y, breaks=seq(10,70,1), col="#0000FF7F", add=T)
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
setwd("~/SchoolWeight")
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
pJohnson(i + 0.5, parms)
pJohnson(i - 0.5, parms)
pJohnson(i, parms)
hist(x)
hist(x, breaks=seq(10,125,1), col="#FF00007F")
pJohnson(15, parms)
pJohnson(20, parms)
pJohnson(18, parms)
pJohnson(16, parms)
pJohnson(15, parms)
low_val <- pJohnson(i-0.5, parms)
low_val <- pJohnson(15.5, parms)
pJohnson(125, parms)
pJohnson(40, parms)
pJohnson(30, parms)
pJohnson(25, parms)
pJohnson(22, parms)
pJohnson(23, parms)
pJohnson(24, parms)
pJohnson(24.5, parms)
pJohnson(25, parms)
low_val <- pJohnson(25, parms)
class(low_val)
low_val
low_val <- try(pJohnson(25, parms))
class(low_val)
low_val
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
high_val
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hoken_theo_data <- data.frame(Year = new_year, Age = new_age, Sex = new_sex, Weight = new_weight, Theo_count = new_theoCount, Diff_count = new_diffCount)
View(hoken_theo_data)
View(hoken_theo_data)
View(hoken_data)
View(hoken_data)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
hoken_theo_data <- data.frame(Year = new_year, Age = new_age, Sex = new_sex, Weight = new_weight, Theo_count = new_theoCount, Diff_count = new_diffCount)
hoken_theo_data <- data.frame(Year = new_year, Age = new_age, Sex = new_sex, Weight = new_weight, Theo_count = new_theoCount)
View(hoken_theo_data)
View(hoken_theo_data)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
write.table(hoken_theo_data, "Hoken_theo_diff.csv", quote=F, col.names=T, append=F)
write.csv(hoken_theo_data, "Hoken_theo_diff.csv", quote=F, col.names=T, append=F)
write.csv(hoken_theo_data, "Hoken_theo_diff.csv", quote=F, append=F)
write.csv(hoken_theo_data, "Hoken_theo_diff.csv", quote=F)
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
source('~/SchoolWeight/SchoolWeight_back.R', encoding = 'UTF-8')
hist(x, breaks=seq(10,125,1), col="#FF00007F")
hist(y, breaks=seq(10,125,1), col="#0000FF7F", add=T)
